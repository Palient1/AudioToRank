version: '3.8'

services:
  # Ollama service (LLM backend)
  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ollama-server
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    # volumes:
    #   - ollama-data:/root/.ollama
    # networks:
    #   - audio-network

    # LLM Agent service
  llm-agent:
    build:
      context: .
      dockerfile: Dockerfile.agent
    container_name: llm-agent
    environment:
      - OLLAMA_API=http://ollama:11434/api/generate
      - MODEL_NAME=${LLM_MODEL}
      - FLASK_PORT=5001
    ports:
      - "5001:5001"
    depends_on:
      - ollama
    # networks:
    #   - audio-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:5001/health" ]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # Main application service
  main-app:
    build:
      context: .
      dockerfile: Dockerfile.main
      args:
        - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
        - PYANNOTE_MODEL=${PYANNOTE_MODEL:-pyannote/speaker-diarization-3.1}
    container_name: audio-to-rank-main
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - AUDIO_FILE=${AUDIO_FILE}
      - PYANNOTE_MODEL=${PYANNOTE_MODEL:-pyannote/speaker-diarization-3.1}
      - LLM_AGENT_HOST=http://llm-agent:5001
    volumes:
      - ./audios:/app/audio_files
      - ./output:/app/output
    depends_on:
      - llm-agent
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    # networks:
    #   - audio-network

volumes:
  ollama-data:

    # networks:
    #   audio-network:
    #     driver: bridge
