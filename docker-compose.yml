version: '3.8'

services:
  # Ollama service (LLM backend)
  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ollama-server
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    ports:
      - "11434:11434"
    # volumes:
    #   - ollama-data:/root/.ollama
    # networks:
    #   - audio-network

  # LLM Agent service
  llm-agent:
    build:
      context: .
      dockerfile: Dockerfile.agent
    container_name: llm-agent
    environment:
      - OLLAMA_API=http://ollama:11434/api/generate
      - MODEL_NAME=${LLM_MODEL}
      - FLASK_PORT=5001
    ports:
      - "5001:5001"
    depends_on:
      - ollama
    # networks:
    #   - audio-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # Main application service
  main-app:
    build:
      context: .
      dockerfile: Dockerfile.main
    container_name: audio-to-rank-main
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - AUDIO_FILE=${AUDIO_FILE}
      - LLM_AGENT_HOST=http://llm-agent:5001
    volumes:
      - ./audios:/app/audio_files
      - ./output:/app/output
    depends_on:
      - llm-agent
    # networks:
    #   - audio-network

volumes:
  ollama-data:

# networks:
#   audio-network:
#     driver: bridge
